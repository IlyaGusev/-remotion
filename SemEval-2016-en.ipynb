{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English Rest Semeval-2016 SB1 Slot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "    \"data_config\": {\n",
    "        \"competition\": \"semeval\",\n",
    "        \"domain\": \"rest\",\n",
    "        \"language\": \"en\",\n",
    "        \"test_filename\": \"/Volumes/My Passport/Datasets/Sentiment/ABSA16/ABSA16_Restaurants_En_Test.xml\",\n",
    "        \"train_filename\": \"/Volumes/My Passport/Datasets/Sentiment/ABSA16/ABSA16_Restaurants_En_Train.xml\"\n",
    "    },\n",
    "    \"model_config\": {\n",
    "        \"is_sequence_predictor\": true,\n",
    "        \"char_dropout_p\": 0.4,\n",
    "        \"char_embedding_dim\": 4,\n",
    "        \"char_function_output_size\": 30,\n",
    "        \"char_max_word_length\": 30,\n",
    "        \"dense_size\": 32,\n",
    "        \"dense_dropout\": 0.4,\n",
    "        \"gram_dropout_p\": 0.4,\n",
    "        \"gram_hidden_size\": 32,\n",
    "        \"rnn_bidirectional\": true,\n",
    "        \"rnn_dropout_p\": 0.5,\n",
    "        \"rnn_hidden_size\": 32,\n",
    "        \"rnn_output_dropout_p\": 0.4,\n",
    "        \"rnn_n_layers\": 2,\n",
    "        \"use_chars\":  false,\n",
    "        \"use_crf\": true,\n",
    "        \"use_pos\": true,\n",
    "        \"use_word_embeddings\": true,\n",
    "        \"word_embedding_dim\": 300,\n",
    "        \"word_embedding_dropout_p\": 0.4,\n",
    "        \"output_size\": 25\n",
    "    },\n",
    "    \"batch_size\": 8,\n",
    "    \"embeddings_filename\": \"semeval-2016-en-rest-fasttext.txt\",\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 0.001,\n",
    "    \"max_length\": 650,\n",
    "    \"output_filename\": \"submission.xml\",\n",
    "    \"patience\": 2,\n",
    "    \"task_type\": \"12\",\n",
    "    \"use_pretrained_embedding\": true,\n",
    "    \"val_size\": 0.2,\n",
    "    \"word_max_length\": 30,\n",
    "    \"model_filename\": \"model.pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4091\n",
      " ,A:8'$M5o9BmO2/6r‘tf%SKuFPTNQX*3)cj4@p!(sRia;nb.G=+yYe\"L?ZxH0–dD#g-kzJWCEUé’7lw1IvhVq\n"
     ]
    }
   ],
   "source": [
    "from src.config import Config\n",
    "from src.parser import get_data\n",
    "\n",
    "config = Config()\n",
    "config.load(\"config.json\")\n",
    "train_data, test_data = get_data(config.data_config)   \n",
    "\n",
    "max_length = max(train_data.get_lengths() + test_data.get_lengths())\n",
    "vocabulary = train_data.get_vocabulary().merge(test_data.get_vocabulary())\n",
    "char_set = train_data.get_char_set()\n",
    "print(vocabulary.size())\n",
    "print(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AMBIENCE#GENERAL': 0, 'DRINKS#PRICES': 1, 'DRINKS#QUALITY': 2, 'DRINKS#STYLE_OPTIONS': 3, 'FOOD#PRICES': 4, 'FOOD#QUALITY': 5, 'FOOD#STYLE_OPTIONS': 6, 'LOCATION#GENERAL': 7, 'RESTAURANT#GENERAL': 8, 'RESTAURANT#MISCELLANEOUS': 9, 'RESTAURANT#PRICES': 10, 'SERVICE#GENERAL': 11}\n",
      "{0: 'AMBIENCE#GENERAL', 1: 'DRINKS#PRICES', 2: 'DRINKS#QUALITY', 3: 'DRINKS#STYLE_OPTIONS', 4: 'FOOD#PRICES', 5: 'FOOD#QUALITY', 6: 'FOOD#STYLE_OPTIONS', 7: 'LOCATION#GENERAL', 8: 'RESTAURANT#GENERAL', 9: 'RESTAURANT#MISCELLANEOUS', 10: 'RESTAURANT#PRICES', 11: 'SERVICE#GENERAL'}\n",
      "Use cuda:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torchcrf/__init__.py:58: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(self.start_transitions, -0.1, 0.1)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torchcrf/__init__.py:59: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(self.end_transitions, -0.1, 0.1)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torchcrf/__init__.py:60: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(self.transitions, -0.1, 0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words in semeval-2016-en-rest-fasttext.txt: 218\n",
      "Epoch: 0, train loss: 4201.781944056919, val loss: 3568.6249321831597\n",
      "Epoch: 1, train loss: 2959.098163713728, val loss: 3188.3396674262153\n",
      "Epoch: 2, train loss: 2820.4223039899553, val loss: 3104.1981065538193\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-93716e982c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditionals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_targets_additionals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditionals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/Remotion/src/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config_filename, train_data, vocabulary, char_set, targets, additionals)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtrain_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Remotion/src/train.py\u001b[0m in \u001b[0;36mprocess_batch\u001b[0;34m(model, batch, optimizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.train import train_model\n",
    "from src.config import get_targets_additionals\n",
    "\n",
    "targets, additionals = get_targets_additionals(train_data)\n",
    "model = train_model(\"config.json\", train_data, vocabulary, char_set, targets, additionals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torchcrf/__init__.py:58: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(self.start_transitions, -0.1, 0.1)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torchcrf/__init__.py:59: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(self.end_transitions, -0.1, 0.1)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torchcrf/__init__.py:60: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  nn.init.uniform(self.transitions, -0.1, 0.1)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torchcrf/__init__.py:284: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  best_tags = [best_last_tag[0]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.config import Config\n",
    "from src.model import load_model\n",
    "from src.train import get_batches\n",
    "from src.semeval_parser import Opinion, Review, Sentence\n",
    "\n",
    "def predict(model_filename, config_filename, test_data, vocabulary, targets, additionals):\n",
    "    config = Config()\n",
    "    config.load(config_filename)\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    model, _ = load_model(model_filename, config_filename, use_cuda)\n",
    "    model.eval()\n",
    "    gram_vector_size =len(test_data.reviews[0].sentences[0][0].vector)\n",
    "    \n",
    "    task_key = config.data_config.competition + \"-\" + config.task_type\n",
    "    test_batches = get_batches(test_data.reviews, vocabulary, char_set, 1, \n",
    "                               config.max_length, config.word_max_length, \n",
    "                               targets[task_key], additionals[task_key])\n",
    "    new_reviews = []\n",
    "    for review, batch in zip(test_data.reviews, test_batches):\n",
    "        new_review = Review(rid=review.rid)\n",
    "        for sentence in review.parsed_sentences:\n",
    "            new_review.parsed_sentences.append(Sentence(sentence.sid, sentence.text))\n",
    "        predictions = model.predict(batch)\n",
    "        \n",
    "        length = sum([int(elem != 0) for elem in batch.word_indices[0].data])\n",
    "        if model.config.use_crf:\n",
    "            review_pred = predictions[0][:length]\n",
    "        else:\n",
    "            review_pred = predictions[0, :length]\n",
    "\n",
    "        tokens = [word for sentence in review.sentences for word in sentence]\n",
    "        aspect = Opinion()\n",
    "        sid = ''\n",
    "        done_opinions = set()\n",
    "        for i, token in enumerate(tokens):\n",
    "            sid = token.sid\n",
    "            pred_class = review_pred[i].cpu().item()\n",
    "            if config.task_type == '12':\n",
    "                if pred_class % 2 == 0 and pred_class != 0 and aspect.is_empty():\n",
    "                    pred_class -= 1\n",
    "                if pred_class % 2 == 1:\n",
    "                    aspect.words.append(token)\n",
    "                    category = rev_categories[(pred_class-1)//2]\n",
    "                    aspect.cat_first, aspect.cat_second = category.split(\"#\")\n",
    "                    aspect.inflate_target()\n",
    "                if pred_class % 2 == 0 and pred_class != 0:\n",
    "                    aspect.words.append(token)\n",
    "                    category = rev_categories[(pred_class-2)//2]\n",
    "                    aspect.cat_first, aspect.cat_second = category.split(\"#\")\n",
    "                    aspect.inflate_target()\n",
    "                if pred_class == 0 and not aspect.is_empty():\n",
    "                    aspect.begin = aspect.words[0].begin\n",
    "                    aspect.end = aspect.words[-1].end\n",
    "                    aspect.inflate_target()\n",
    "                    for sentence in new_review.parsed_sentences:\n",
    "                        if sentence.sid == token.sid:\n",
    "                            sentence.aspects.append(aspect)\n",
    "                    new_review.aspects.append(aspect)\n",
    "                    aspect = Opinion()\n",
    "            elif config.task_type == '3':\n",
    "                if not token.opinions:\n",
    "                    continue\n",
    "                for opinion in token.opinions:\n",
    "                    if opinion not in done_opinions:\n",
    "                        aspect = Opinion(begin=opinion.begin, end=opinion.end, \n",
    "                                         polarity=pred_class-1, cat_first=opinion.cat_first,\n",
    "                                         cat_second=opinion.cat_second,\n",
    "                                         target=opinion.target.replace('\"', \"'\").replace('&', '#'))\n",
    "                        done_opinions.add(opinion)\n",
    "                        for sentence in new_review.parsed_sentences:\n",
    "                            if sentence.sid == token.sid:\n",
    "                                sentence.aspects.append(aspect)\n",
    "                        new_review.aspects.append(aspect)\n",
    "        if config.task_type == '12' and not aspect.is_empty():\n",
    "            for sentence in new_review.parsed_sentences:\n",
    "                if sentence.sid == token.sid:\n",
    "                    sentence.aspects.append(aspect)\n",
    "            new_review.aspects.append(aspect)\n",
    "        new_reviews.append(new_review)\n",
    "    \n",
    "    xml = '<?xml version=\"1.0\" ?>\\n'\n",
    "    xml += '<Reviews>\\n'\n",
    "    for review in new_reviews:\n",
    "        xml += review.to_xml()\n",
    "    xml += '</Reviews>\\n'\n",
    "    with open(config.output_filename, \"w\", encoding='utf-8') as f:\n",
    "        f.write(xml)\n",
    "\n",
    "predict(\"model.pt\", \"config.json\", test_data, vocabulary, targets, additionals)\n",
    "!head -n 1000 submission.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed words: 0, intersection: 0, unknown words:4091\n",
      "Parsed words: 100000, intersection: 3487, unknown words:604\n",
      "Parsed words: 200000, intersection: 3645, unknown words:446\n",
      "Parsed words: 300000, intersection: 3706, unknown words:385\n",
      "Parsed words: 400000, intersection: 3738, unknown words:353\n",
      "Parsed words: 500000, intersection: 3760, unknown words:331\n",
      "Parsed words: 600000, intersection: 3775, unknown words:316\n",
      "Parsed words: 700000, intersection: 3782, unknown words:309\n",
      "Parsed words: 800000, intersection: 3799, unknown words:292\n",
      "Parsed words: 900000, intersection: 3809, unknown words:282\n",
      "Parsed words: 1000000, intersection: 3818, unknown words:273\n",
      "Parsed words: 1100000, intersection: 3826, unknown words:265\n",
      "Parsed words: 1200000, intersection: 3832, unknown words:259\n",
      "Parsed words: 1300000, intersection: 3835, unknown words:256\n",
      "Parsed words: 1400000, intersection: 3840, unknown words:251\n",
      "Parsed words: 1500000, intersection: 3844, unknown words:247\n",
      "Parsed words: 1600000, intersection: 3851, unknown words:240\n",
      "Parsed words: 1700000, intersection: 3853, unknown words:238\n",
      "Parsed words: 1800000, intersection: 3855, unknown words:236\n",
      "Parsed words: 1900000, intersection: 3858, unknown words:233\n",
      "Parsed words: 2000000, intersection: 3859, unknown words:232\n",
      "Parsed words: 2100000, intersection: 3864, unknown words:227\n",
      "Parsed words: 2200000, intersection: 3864, unknown words:227\n",
      "Parsed words: 2300000, intersection: 3867, unknown words:224\n",
      "Parsed words: 2400000, intersection: 3872, unknown words:219\n",
      "Parsed words: 2500000, intersection: 3872, unknown words:219\n",
      "Unknown examples: ['parmigana', 'tastly', '13', '!)', '74th', '...).', ').', '15', '20', 'sashmi', 'alked', '\")', 'monring', '%.', '30', '2nd', 'tortelini', 'teareyed', 'whealthy', '<pad>', '95', 'chimmichuri', ')!', '26', 'hollondaise', 'mediorce', 'chuwam', '5', '00', '\",', '($', 'afortune', \".'\", '2', 'korbett', 'dhosas', 'cassuolet', '!!!!!', '40', 'celebrtating', '27', '.\"', '24', '99', 'bruschettas', '...)', '8', 'panchetta', '12', 'zabars', 'avodcao', 'prixe', 'curtious', ':):):)', 'steake', '!!!!!!!!', '+.', 'uppereast', '100', '\"...', '14', '32nd', 'mocaramel', 'omikase', '\"?', 'winelist', 'raouls', 'tramezzinis', '57th', '70', \"',\", '$$$$.', 'mioposto', 'imazingly', 'dosut', 'honwy', 'quacamole', 'southglen', 'sushimi', 'fickled', '9', '...!', 'slamwich', 'dumbfoundingly', '--', 'jewelbako', 'bestt', '6', '23rd', 'vanison', '??', 'nakgi', '.,', 'fornino', '65', '7', 'arrabiatta', '),', ';', '55']\n"
     ]
    }
   ],
   "source": [
    "from src.embeddings import shrink_w2v\n",
    "ALL_EMBEDDINGS_FILENAME = \"/Volumes/My Passport/Models/Vectors/FastText/wiki.en.vec\"\n",
    "EMBEDDINGS_FILENAME = \"semeval-2016-en-rest-fasttext.txt\"\n",
    "shrink_w2v(ALL_EMBEDDINGS_FILENAME, vocabulary, 10000, EMBEDDINGS_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
