{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from json...\n",
      "Num of reviews: 201\n",
      "Num of opinions: 4361\n",
      "Max review length: 272\n",
      "[<Word \"День\" from 0 to 4 with opinion None at 0x11f7b32b0>, <Word \"8\" from 5 to 6 with opinion None at 0x11f7b32e8>, <Word \"-\" from 6 to 7 with opinion None at 0x11f7b3320>, <Word \"го\" from 7 to 9 with opinion None at 0x11f7b3358>, <Word \"марта\" from 10 to 15 with opinion None at 0x11f7b3390>, <Word \"прошёл\" from 16 to 22 with opinion None at 0x11f7b33c8>, <Word \",\" from 22 to 23 with opinion None at 0x11f7b3400>, <Word \"можно\" from 24 to 29 with opinion None at 0x11f7b3438>, <Word \"и\" from 30 to 31 with opinion None at 0x11f7b3470>, <Word \"итоги\" from 32 to 37 with opinion None at 0x11f7b34a8>, <Word \"подвести\" from 38 to 46 with opinion None at 0x11f7b34e0>, <Word \".\" from 46 to 47 with opinion None at 0x11f7b3518>]\n",
      "[<PosTaggedWord \"День\", NOUN#Case=Acc|Gender=Masc|Number=Sing, [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x11e922b00>, <PosTaggedWord \"8\", NUM#NumForm=Digit, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x11e922b70>, <PosTaggedWord \"-\", PUNCT#_, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x11e922ba8>, <PosTaggedWord \"го\", ADP#_, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x11e922be0>, <PosTaggedWord \"марта\", NOUN#Case=Gen|Gender=Masc|Number=Sing, [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x11e922c18>, <PosTaggedWord \"прошёл\", VERB#Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] at 0x11e922c50>, <PosTaggedWord \",\", PUNCT#_, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x11e922c88>, <PosTaggedWord \"можно\", ADJ#Degree=Pos|Gender=Neut|Number=Sing|Variant=Short, [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1] at 0x11e922cc0>, <PosTaggedWord \"и\", PART#_, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x11e922cf8>, <PosTaggedWord \"итоги\", NOUN#Case=Acc|Gender=Masc|Number=Plur, [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x11e922d30>, <PosTaggedWord \"подвести\", VERB#VerbForm=Inf, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1] at 0x11e922d68>, <PosTaggedWord \".\", PUNCT#_, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x11e922da0>]\n",
      "Num of reviews: 203\n",
      "Num of opinions: 5117\n",
      "Max review length: 263\n",
      "[<Word \"По\" from 0 to 2 with opinion None at 0x1234f7ba8>, <Word \"совету\" from 3 to 9 with opinion None at 0x1234f7be0>, <Word \"друзей\" from 10 to 16 with opinion None at 0x1234f7c18>, <Word \"посетили\" from 17 to 25 with opinion None at 0x1234f7c50>, <Word \"данное\" from 26 to 32 with opinion None at 0x1234f7c88>, <Word \"заведение\" from 33 to 42 with opinion <Aspect 33:42 0 Whole 1 at 0x1225ebe80> at 0x1225ebf98>, <Word \".\" from 42 to 43 with opinion None at 0x1234f7cc0>]\n",
      "[<PosTaggedWord \"По\", ADP#_, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x1225ebcc0>, <PosTaggedWord \"совету\", NOUN#Case=Loc|Gender=Masc|Number=Sing, [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x1225ebd30>, <PosTaggedWord \"друзей\", NOUN#Case=Gen|Gender=Masc|Number=Plur, [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x1225ebd68>, <PosTaggedWord \"посетили\", VERB#Mood=Ind|Number=Plur|Tense=Past|VerbForm=Fin|Voice=Act, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] at 0x1225ebda0>, <PosTaggedWord \"данное\", ADJ#Case=Acc|Degree=Pos|Gender=Neut|Number=Sing, [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x1225ebdd8>, <PosTaggedWord \"заведение\", NOUN#Case=Acc|Gender=Neut|Number=Sing, [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x1225ebe10>, <PosTaggedWord \".\", PUNCT#_, [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1] at 0x1202b64e0>]\n",
      "9027\n",
      " сАVЖ(DAЗ\"umМШЬщЧЮtыoъГфHб»йnгxcЭCОЛюjвчRL&Е:MенEДvЩ/Бr7цузdшпФ>тьkё=1pТ—sд.N3мС?РокВ№ЫжНК6ЦяgBаI+«эх0,иe8SGyliПP;4–ХрbW'ЯhИУ…f2!%a-9л)T5\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from rnnmorph.data_preparation.grammeme_vectorizer import GrammemeVectorizer\n",
    "\n",
    "from src.sentirueval_parser import SentiRuEvalDataset\n",
    "\n",
    "def sentirueval_get_data(filename):\n",
    "    data = SentiRuEvalDataset()\n",
    "    if filename.endswith(\"xml\"):\n",
    "        data.parse(filename, \"gram_output.json\")\n",
    "    elif filename.endswith(\"json\"):\n",
    "        data.load(filename)\n",
    "    else:\n",
    "        assert False\n",
    "    print(\"Num of reviews: \" + str(len(data.reviews)))\n",
    "    print(\"Num of opinions: \" + str(data.get_opinion_count()))\n",
    "    print(\"Max review length: \" + str(max(data.get_lengths())))\n",
    "    print(data.tokenized_reviews[0][0])\n",
    "    print(data.pos_tagged_reviews[0][0])\n",
    "    return data\n",
    "\n",
    "TRAIN_FILENAME = \"/Users/ilya-gusev/Projects/Remotion/ABSA/SentiRuEval-2015/SentiRuEval_rest_markup_train.xml\"\n",
    "TEST_FILENAME = \"/Users/ilya-gusev/Projects/Remotion/ABSA/SentiRuEval-2015/SentiRuEval_rest_markup_test.xml\"\n",
    "PICKLED_TRAIN_FILENAME = \"senti-train.json\"\n",
    "PICKLED_TEST_FILENAME = \"senti-test.json\"\n",
    "GRAMMEME_VECTORIZER_FILENAME = \"gram_output.json\"\n",
    "\n",
    "reload = False\n",
    "if not os.path.exists(PICKLED_TRAIN_FILENAME) or not os.path.exists(PICKLED_TEST_FILENAME) or reload:\n",
    "    print(\"Loading from xml...\")\n",
    "    train_data = sentirueval_get_data(TRAIN_FILENAME)\n",
    "    test_data = sentirueval_get_data(TEST_FILENAME)\n",
    "    train_data.save(PICKLED_TRAIN_FILENAME)\n",
    "    test_data.save(PICKLED_TEST_FILENAME)\n",
    "else:\n",
    "    print(\"Loading from json...\")\n",
    "    train_data = sentirueval_get_data(PICKLED_TRAIN_FILENAME)\n",
    "    test_data = sentirueval_get_data(PICKLED_TEST_FILENAME)\n",
    "\n",
    "max_length = max(train_data.get_lengths() + test_data.get_lengths())\n",
    "grammeme_vectorizer = GrammemeVectorizer(GRAMMEME_VECTORIZER_FILENAME)\n",
    "vocabulary = train_data.get_vocabulary().merge(test_data.get_vocabulary())\n",
    "char_set = train_data.get_char_set()\n",
    "print(vocabulary.size())\n",
    "print(char_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed words: 0, intersection: 0, unknown words:9027\n",
      "Parsed words: 100000, intersection: 6797, unknown words:2230\n",
      "Parsed words: 200000, intersection: 7577, unknown words:1450\n",
      "Parsed words: 300000, intersection: 7893, unknown words:1134\n",
      "Parsed words: 400000, intersection: 8028, unknown words:999\n",
      "Parsed words: 500000, intersection: 8127, unknown words:900\n",
      "Parsed words: 600000, intersection: 8183, unknown words:844\n",
      "Parsed words: 700000, intersection: 8227, unknown words:800\n",
      "Parsed words: 800000, intersection: 8266, unknown words:761\n",
      "Parsed words: 900000, intersection: 8284, unknown words:743\n",
      "Parsed words: 1000000, intersection: 8299, unknown words:728\n",
      "Parsed words: 1100000, intersection: 8316, unknown words:711\n",
      "Parsed words: 1200000, intersection: 8333, unknown words:694\n",
      "Parsed words: 1300000, intersection: 8347, unknown words:680\n",
      "Parsed words: 1400000, intersection: 8353, unknown words:674\n",
      "Parsed words: 1500000, intersection: 8364, unknown words:663\n",
      "Parsed words: 1600000, intersection: 8372, unknown words:655\n",
      "Parsed words: 1700000, intersection: 8378, unknown words:649\n",
      "Parsed words: 1800000, intersection: 8384, unknown words:643\n",
      "Parsed words: 1900000, intersection: 8387, unknown words:640\n",
      "Parsed words: 2000000, intersection: 8393, unknown words:634\n",
      "Parsed words: 2100000, intersection: 8396, unknown words:631\n",
      "Parsed words: 2200000, intersection: 8401, unknown words:626\n",
      "Parsed words: 2300000, intersection: 8403, unknown words:624\n",
      "Parsed words: 2400000, intersection: 8411, unknown words:616\n",
      "Parsed words: 2500000, intersection: 8417, unknown words:610\n",
      "Parsed words: 2600000, intersection: 8420, unknown words:607\n",
      "Parsed words: 2700000, intersection: 8424, unknown words:603\n",
      "Parsed words: 2800000, intersection: 8428, unknown words:599\n",
      "Parsed words: 2900000, intersection: 8429, unknown words:598\n",
      "Parsed words: 3000000, intersection: 8431, unknown words:596\n",
      "Parsed words: 3100000, intersection: 8432, unknown words:595\n",
      "Parsed words: 3200000, intersection: 8432, unknown words:595\n",
      "Parsed words: 3300000, intersection: 8437, unknown words:590\n",
      "Parsed words: 3400000, intersection: 8439, unknown words:588\n",
      "Parsed words: 3500000, intersection: 8441, unknown words:586\n",
      "Parsed words: 3600000, intersection: 8444, unknown words:583\n",
      "Parsed words: 3700000, intersection: 8444, unknown words:583\n",
      "Parsed words: 3800000, intersection: 8446, unknown words:581\n",
      "Parsed words: 3900000, intersection: 8450, unknown words:577\n",
      "Parsed words: 4000000, intersection: 8455, unknown words:572\n",
      "Parsed words: 4100000, intersection: 8455, unknown words:572\n",
      "Parsed words: 4200000, intersection: 8455, unknown words:572\n",
      "Parsed words: 4300000, intersection: 8456, unknown words:571\n",
      "Parsed words: 4400000, intersection: 8459, unknown words:568\n",
      "Parsed words: 4500000, intersection: 8459, unknown words:568\n",
      "Parsed words: 4600000, intersection: 8460, unknown words:567\n",
      "Parsed words: 4700000, intersection: 8461, unknown words:566\n",
      "Parsed words: 4800000, intersection: 8461, unknown words:566\n",
      "Parsed words: 4900000, intersection: 8463, unknown words:564\n",
      "Parsed words: 5000000, intersection: 8464, unknown words:563\n",
      "Parsed words: 5100000, intersection: 8465, unknown words:562\n",
      "Parsed words: 5200000, intersection: 8467, unknown words:560\n",
      "Parsed words: 5300000, intersection: 8472, unknown words:555\n",
      "Parsed words: 5400000, intersection: 8472, unknown words:555\n",
      "Parsed words: 5500000, intersection: 8473, unknown words:554\n",
      "Parsed words: 5600000, intersection: 8474, unknown words:553\n",
      "Parsed words: 5700000, intersection: 8477, unknown words:550\n",
      "Parsed words: 5800000, intersection: 8478, unknown words:549\n",
      "Parsed words: 5900000, intersection: 8478, unknown words:549\n",
      "Parsed words: 6000000, intersection: 8478, unknown words:549\n",
      "Parsed words: 6100000, intersection: 8479, unknown words:548\n",
      "Parsed words: 6200000, intersection: 8480, unknown words:547\n",
      "Parsed words: 6300000, intersection: 8481, unknown words:546\n",
      "Parsed words: 6400000, intersection: 8482, unknown words:545\n",
      "Parsed words: 6500000, intersection: 8482, unknown words:545\n",
      "Parsed words: 6600000, intersection: 8484, unknown words:543\n",
      "Parsed words: 6700000, intersection: 8486, unknown words:541\n",
      "Parsed words: 6800000, intersection: 8486, unknown words:541\n",
      "Parsed words: 6900000, intersection: 8486, unknown words:541\n",
      "Parsed words: 7000000, intersection: 8486, unknown words:541\n",
      "Parsed words: 7100000, intersection: 8486, unknown words:541\n",
      "Parsed words: 7200000, intersection: 8486, unknown words:541\n",
      "Parsed words: 7300000, intersection: 8487, unknown words:540\n"
     ]
    }
   ],
   "source": [
    "from src.embeddings import shrink_w2v\n",
    "shrink_w2v(\"/Volumes/My Passport/Models/russian-big-w2v.txt\", vocabulary, 10000, \"sentirueval2015-w2v.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "    \"char_dropout_p\": 0.2,\n",
    "    \"char_embedding_dim\": 10,\n",
    "    \"char_function_output_size\": 100,\n",
    "    \"char_max_word_length\": 30,\n",
    "    \"dense_size\": 50,\n",
    "    \"gram_dropout_p\": 0.3,\n",
    "    \"gram_hidden_size\": 20,\n",
    "    \"rnn_bidirectional\": true,\n",
    "    \"rnn_dropout_p\": 0.5,\n",
    "    \"rnn_hidden_size\": 80,\n",
    "    \"rnn_n_layers\": 3,\n",
    "    \"use_chars\": false,\n",
    "    \"use_crf\": false,\n",
    "    \"use_pos\": true,\n",
    "    \"word_embedding_dim\": 500,\n",
    "    \"word_embedding_dropout_p\": 0.3,\n",
    "    \"output_size\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words in sentirueval2015-w2v.txt: 539\n",
      "RemotionRNN(\n",
      "  (embedding): Embedding(9027, 500)\n",
      "  (embedding_dropout): Dropout(p=0.3)\n",
      "  (grammeme_dense): Linear(in_features=52, out_features=20, bias=True)\n",
      "  (grammeme_activation): ReLU()\n",
      "  (grammeme_dropout): Dropout(p=0.3)\n",
      "  (rnn): LSTM(520, 80, num_layers=3, dropout=0.5, bidirectional=True)\n",
      "  (dense): Linear(in_features=160, out_features=50, bias=True)\n",
      "  (dense_activation): ReLU()\n",
      "  (output): Linear(in_features=50, out_features=3, bias=True)\n",
      ")\n",
      "Epoch: 0, train loss: 1510.5465057373046, val loss: 582.7085393269857\n",
      "Epoch: 1, train loss: 675.907568359375, val loss: 528.9339396158854\n",
      "Epoch: 2, train loss: 648.1932159423828, val loss: 494.6009801228841\n",
      "Epoch: 3, train loss: 563.8332427978515, val loss: 369.0206019083659\n",
      "Epoch: 4, train loss: 422.1195556640625, val loss: 295.36036682128906\n",
      "Epoch: 5, train loss: 371.94013671875, val loss: 277.1470832824707\n",
      "Epoch: 6, train loss: 352.84391632080076, val loss: 258.60805638631183\n",
      "Epoch: 7, train loss: 341.4280860900879, val loss: 251.19236373901367\n",
      "Epoch: 8, train loss: 333.70625457763674, val loss: 242.0011952718099\n",
      "Epoch: 9, train loss: 326.87804946899416, val loss: 239.42455546061197\n",
      "Epoch: 10, train loss: 322.00917587280276, val loss: 233.09984270731607\n",
      "Epoch: 11, train loss: 317.22248458862305, val loss: 230.92584164937338\n",
      "Epoch: 12, train loss: 315.07564544677734, val loss: 228.86311467488608\n",
      "Epoch: 13, train loss: 310.60089645385744, val loss: 228.31035804748535\n",
      "Epoch: 14, train loss: 310.41064224243166, val loss: 225.3156935373942\n",
      "Epoch: 15, train loss: 305.71365585327146, val loss: 224.52581659952799\n",
      "Epoch: 16, train loss: 299.6034767150879, val loss: 224.32339223225912\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from src.model import Config\n",
    "from src.train import train_model\n",
    "\n",
    "def target_function_a(word):\n",
    "    if word.opinion is None:\n",
    "        return 0\n",
    "    if word.opinion.type != 0:\n",
    "        return 0\n",
    "    if word.opinion.mark != 0:\n",
    "        return 0\n",
    "    if word.opinion.words[0].text == word.text:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "def target_function_b(word):\n",
    "    if word.opinion is None:\n",
    "        return 0\n",
    "    if word.opinion.mark != 0:\n",
    "        return 0\n",
    "    opinion_type = word.opinion.type\n",
    "    if word.opinion.words[0].text == word.text:\n",
    "        return 2*opinion_type + 1\n",
    "    return 2*opinion_type + 2\n",
    "\n",
    "max_word_length = 30\n",
    "model = train_model(\n",
    "    \"config.json\",\n",
    "    \"model.pt\",\n",
    "    train_data.pos_tagged_reviews, \n",
    "    vocabulary, \n",
    "    char_set,\n",
    "    target_function=target_function_a,\n",
    "    epochs=40,\n",
    "    val_size=0.2,\n",
    "    max_length=max_length,\n",
    "    max_word_length=max_word_length,\n",
    "    use_pretrained_embeddings=True,\n",
    "    patience=2,\n",
    "    embeddings_filename=\"sentirueval2015-w2v.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from src.model import load_model\n",
    "from src.train import get_batches, predict_batch\n",
    "from src.sentirueval_parser import Aspect, Review\n",
    "\n",
    "def form_submission(model,\n",
    "                    test_data, \n",
    "                    vocabulary, \n",
    "                    gram_vector_size,\n",
    "                    max_length=max_length,\n",
    "                    max_word_length=max_word_length,\n",
    "                    output_filename=\"submission.xml\"):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    test_batches = get_batches(test_data.pos_tagged_reviews, vocabulary, char_set, \n",
    "                               gram_vector_size, 1, max_length, max_word_length, target_function_b)\n",
    "    new_reviews = []\n",
    "    for tokenized_review, (review, (text_batch, gram_batch, char_batch, y)) in\\\n",
    "            zip(test_data.tokenized_reviews, zip(test_data.reviews, test_batches)):\n",
    "        new_review = Review(rid=review.rid, text=review.text)\n",
    "        model.eval()\n",
    "        predictions = predict_batch(model, text_batch, gram_batch, char_batch, use_cuda)\n",
    "        length = sum([int(elem != 0) for elem in text_batch[0].data])\n",
    "        if model.config.use_crf:\n",
    "            review_pred = predictions[0][:length]\n",
    "        else:\n",
    "            review_pred = predictions[0, :length]\n",
    "        \n",
    "        tokens = [word for sentence in tokenized_review for word in sentence]\n",
    "        type_class = None\n",
    "        aspect = Aspect(mark=0, aspect_type=0)\n",
    "        for i, token in enumerate(tokens):\n",
    "            pred_class = review_pred[i].cpu().item()\n",
    "            if pred_class % 2 == 0 and pred_class != 0 and aspect.is_empty():\n",
    "                pred_class -= 1\n",
    "            if pred_class % 2 == 1:\n",
    "                aspect.words.append(token)\n",
    "                aspect.type =(pred_class-1)//2\n",
    "                aspect.inflate_target()\n",
    "            if pred_class % 2 == 0 and pred_class != 0:\n",
    "                aspect.words.append(token)\n",
    "                aspect.type = pred_class//2\n",
    "                aspect.inflate_target()\n",
    "            if pred_class == 0 and not aspect.is_empty():\n",
    "                aspect.begin = aspect.words[0].begin\n",
    "                aspect.end = aspect.words[-1].end\n",
    "                aspect.inflate_target()\n",
    "                new_review.aspects.append(aspect)\n",
    "                aspect = Aspect(mark=0, aspect_type=0)\n",
    "        new_reviews.append(new_review)\n",
    "    \n",
    "    xml = '<?xml version=\"1.0\" ?>\\n'\n",
    "    xml += '<reviews>\\n'\n",
    "    for review in new_reviews:\n",
    "        xml += review.to_xml()\n",
    "    xml += '</reviews>\\n'\n",
    "    with open(output_filename, \"w\", encoding='utf-8') as f:\n",
    "        f.write(xml)\n",
    "\n",
    "model, _ = load_model(\"model.pt\", \"config.json\", torch.cuda.is_available())\n",
    "form_submission(model, test_data,vocabulary, grammeme_vectorizer.grammemes_count())\n",
    "!head -n 100 submission.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 eval/eval1.py -g ABSA/SentiRuEval-2015/SentiRuEval_rest_markup_test.xml -t submission.xml -a a -w weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
